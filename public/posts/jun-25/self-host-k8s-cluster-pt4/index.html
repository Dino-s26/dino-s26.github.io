<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Self-host Kubernetes Cluster Pt.4 | Dino-s26</title><meta name=keywords content="kubernetes,k8s,microk8s,self-host,tech,learning"><meta name=description content="Intermezzo


Meowdy &#x1f63a;, been while not update the article series and welcome to the 4th Part of self-host kubernetes !
This part we will setup the Low spec Production read K8s Clusters (usually used for Edge / IoT cases). I was planned to use microk8s by ubuntu but since the k0s (K-Zero-S) just graduate from incubation, I would like to try that one
Before we start, here are the specs I use for the Server I used as the Control-Plane and worker"><meta name=author content="Dino"><link rel=canonical href=http://localhost:1313/posts/jun-25/self-host-k8s-cluster-pt4/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/jun-25/self-host-k8s-cluster-pt4/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-B0ZKDHC64T"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-B0ZKDHC64T")</script><meta property="og:url" content="http://localhost:1313/posts/jun-25/self-host-k8s-cluster-pt4/"><meta property="og:site_name" content="Dino-s26"><meta property="og:title" content="Self-host Kubernetes Cluster Pt.4"><meta property="og:description" content="Intermezzo Meowdy üò∫, been while not update the article series and welcome to the 4th Part of self-host kubernetes !
This part we will setup the Low spec Production read K8s Clusters (usually used for Edge / IoT cases). I was planned to use microk8s by ubuntu but since the k0s (K-Zero-S) just graduate from incubation, I would like to try that one
Before we start, here are the specs I use for the Server I used as the Control-Plane and worker"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-30T00:00:00+00:00"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="K8s"><meta property="article:tag" content="Microk8s"><meta property="article:tag" content="Self-Host"><meta property="article:tag" content="Tech"><meta property="article:tag" content="Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Self-host Kubernetes Cluster Pt.4"><meta name=twitter:description content="Intermezzo


Meowdy &#x1f63a;, been while not update the article series and welcome to the 4th Part of self-host kubernetes !
This part we will setup the Low spec Production read K8s Clusters (usually used for Edge / IoT cases). I was planned to use microk8s by ubuntu but since the k0s (K-Zero-S) just graduate from incubation, I would like to try that one
Before we start, here are the specs I use for the Server I used as the Control-Plane and worker"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Self-host Kubernetes Cluster Pt.4","item":"http://localhost:1313/posts/jun-25/self-host-k8s-cluster-pt4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Self-host Kubernetes Cluster Pt.4","name":"Self-host Kubernetes Cluster Pt.4","description":"Intermezzo Meowdy \u0026#x1f63a;, been while not update the article series and welcome to the 4th Part of self-host kubernetes !\nThis part we will setup the Low spec Production read K8s Clusters (usually used for Edge / IoT cases). I was planned to use microk8s by ubuntu but since the k0s (K-Zero-S) just graduate from incubation, I would like to try that one\nBefore we start, here are the specs I use for the Server I used as the Control-Plane and worker\n","keywords":["kubernetes","k8s","microk8s","self-host","tech","learning"],"articleBody":"Intermezzo Meowdy üò∫, been while not update the article series and welcome to the 4th Part of self-host kubernetes !\nThis part we will setup the Low spec Production read K8s Clusters (usually used for Edge / IoT cases). I was planned to use microk8s by ubuntu but since the k0s (K-Zero-S) just graduate from incubation, I would like to try that one\nBefore we start, here are the specs I use for the Server I used as the Control-Plane and worker\nUbuntu Minimized, Version 22.04.5 Storage 20GB RAM 2GB with 4GB Swap k0sctl Install and Configure k0s Cluster I assume you already know how to prepare the server on your own, so I will just jump to the installation part here, so let‚Äôs start with installing the k0sctl tool, which we will use to bootstrap our cluster.\nHead to the Github of k0s to install the k0sctl\nSSH to your server and do update first before proceed with installation os k0s, since I use ubuntu usually I do as follows:\nsudo apt update \u0026\u0026 sudo apt upgrade -y ## for minimized version usually you need to install additional tools like nano, curl, etc etc ... so dont forget to install it sudo apt install nano curl -y Wait for the k0s installation complete, should the installation complete will return similar output as follow: Next we will create bootstrap file for the controller, as follows: mkdir -p /etc/k0s k0s config create \u003e /etc/k0s/k0s.yaml ## as note, since we create file on /etc/ folder which manage by root, change to root user first before create the k0s.yaml or simply put the k0s.yaml somewhere first then copy with sudo for easier way of executable Once the k0s.yaml config file ready, you can check some of the configuration first, as follows: apiVersion: k0s.k0sproject.io/v1beta1 kind: ClusterConfig metadata: name: k0s namespace: kube-system spec: api: address: 192.168.1.81 ca: certificatesExpireAfter: 8760h0m0s expiresAfter: 87600h0m0s k0sApiPort: 9443 port: 6443 sans: - 192.168.1.81 - fe80::be24:11ff:fe41:f0a0 - k0s. ## custom sans I add here controllerManager: {} extensions: helm: concurrencyLevel: 5 installConfig: users: etcdUser: etcd kineUser: kube-apiserver konnectivityUser: konnectivity-server kubeAPIserverUser: kube-apiserver kubeSchedulerUser: kube-scheduler konnectivity: adminPort: 8133 agentPort: 8132 network: clusterDomain: cluster.local dualStack: enabled: false kubeProxy: iptables: minSyncPeriod: 0s syncPeriod: 0s ipvs: minSyncPeriod: 0s syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s udpTimeout: 0s metricsBindAddress: 0.0.0.0:10249 mode: iptables nftables: minSyncPeriod: 0s syncPeriod: 0s kuberouter: autoMTU: true hairpin: Enabled metricsPort: 8080 nodeLocalLoadBalancing: enabled: false envoyProxy: apiServerBindPort: 7443 konnectivityServerBindPort: 7132 type: EnvoyProxy podCIDR: 10.244.0.0/16 provider: kuberouter serviceCIDR: 10.96.0.0/12 scheduler: {} storage: etcd: ca: certificatesExpireAfter: 8760h0m0s expiresAfter: 87600h0m0s peerAddress: 192.168.1.81 type: etcd telemetry: enabled: false on step 4 I just make adjustment to add additional SANS, since I will have the cluster be accessable with domain I add, I still not sure will it work or not since this is the first time I setup the k0s Cluster\nOnce the configuratio is generated and adjusted, let‚Äôs start the control-plane with following command: sudo k0s install controller -c /etc/k0s/k0s.yaml --enable-worker --no-taints sudo k0s start you will see nothing on your terminal, but if you do see some error, try to check the error first before continue with next steps\nLet‚Äôs verify if the installation really complete with following commands: sudo k0s status you will see similar output as follow:\nOkay, so now we have the control-plane configured and running, let‚Äôs verify some of the kubernetes services with following commands: sudo k0s kubectl get nodes -A sudo k0s kubectl cluster-info sudo k0s kubectl get all -A -owide it should output something similar as follows:\nas we can see the cluster is already properly setup, now let‚Äôs get the kubeconfig to be use from our local machine\nTo get the kubeconfig from k0s, use the following command: sudo k0s kubeconfig admin it will output the kubeconfig configuration to your terminal, the put is somehere in your local machine (e.g. ~/.kube/config)\nTest the kubectl from your local machine after you update the kubeconfig, you should be able to access it like this Install Load Balancer So we just finished installed our K8s Cluster with K0s, but we dont actually finished yet with the installation, usually if we do setup on the cloud provider we will have the load balancer automatically provisioned to our cluster, but not quite on the self-host setup.\nNow, let‚Äôs configure Load Balancer to our K8s cluster, we will use MetalLB as our Baremetal Load Balancer for L2 Traffic that will go-in and go-out of our k8s cluster.\nFirst let‚Äôs create new namespace called ‚Äúmetallb-system‚Äù, once created we will deploy the metallb with kustomize in order for easier management of the resource we will do, use the following command to create the namespace: kubectl create namespace metallb-system ## To verify the namespace creation use following command kubectl get namespaces the output should be similar like following capture:\nNext we will create folder containing our kustomize resources will be deployed, in this case for metallb requirement, following are the required resource will be created: metallb-k0s/ ‚îú‚îÄ‚îÄ ipaddresspool.yaml ‚îú‚îÄ‚îÄ kustomization.yaml ‚îî‚îÄ‚îÄ l2advertisement.yaml Before continue, as we can see we have 3 yaml files, which are ‚Äòipaddresspool.yaml, kustomization.yaml, l2advertisement.yaml‚Äô, let me explain a bit why we need these 3 files to install metallb to our cluster ‚Ä¶\n[ kustomization.yaml ] This file will read all our customization we need in-order to install the MetalLB to our cluster, which we will break down in a bit how the structure looks like as follows:\n### kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 namespace: metallb-system resources: - github.com/metallb/metallb/config/native?ref=v0.15.2 - ipaddresspool.yaml - l2advertisement.yaml as we can see this is the structure for kustomization.yaml, which we will call necessary resources like the metallb installation resources, ipaddresspool.yaml and also l2advertisement.yaml\n[ ipaddresspool.yaml ] This file will be our allocation IP Address used for the Load Balancer, it can be IP Public or IP Private, depending on your setup, but for this case since this is self-hosted on my home lab, I will be using IP Private, the structure looks likes as follwos:\napiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: k0s-metallb-pool namespace: metallb-system spec: addresses: - 192.168.1.85-192.168.1.90 #\u003c-- kindly change to your IP Address allocation autoAssign: true avoidBuggyIPs: true as we can see this is the structure of ipaddresspool.yaml, in this case we allocate IP Private from 192.168.1.85 - 192.168.1.90, also we assign the name of the IPAddressPool and namespace where we will deploy it.\nthere will be question, why there is ‚ÄúautoAssign‚Äù and ‚ÄúavoidBuggyIPs‚Äù in the configuration.\n‚ÄúautoAssign‚Äù is used when there is ‚Äúservices‚Äù that configured as LoadBalancer Type, for example like following:\napiVersion: v1 kind: Service metadata: name: nginx-ingress-controller namespace: nginx-ingress spec: allocateLoadBalancerNodePorts: true ClusterIP: x.x.x.x ClusterIPs: - x.x.x.x externalTrafficPolicy: Cluster internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: web nodePort: 32732 port: 80 protocol: TCP targetPort: web - name: websecure nodePort: 32180 port: 443 protocol: TCP targetPort: websecure selector: app.kubernetes.io/name: nginx-ingress-controller type: LoadBalancer # \u003c-- This is the part MetalLB will see when assigning IP This is only example what will be check by MetalLB before auto assign the IP Address, we can also allocate it manually but will need some manual adjustment to it, so for now let‚Äôs leave it as ‚ÄúautoAssign‚Äù\nfor the ‚ÄúavoidBuggyIPs‚Äù, it is related to the network devices that have protection against Smurf IP Address like ‚Äú.0‚Äù or ‚Äú.255‚Äù, the detail can be read here\n[ l2advertisement.yaml ] This file will handle all the L2 communication going-in and going-out of the cluster, since we don‚Äôt use BGP for this setup, we only configure L2-Advertisement only, here are the structure looks like:\napiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: k0s-metallb-l2adv namespace: metallb-system spec: ipAddressPools: - k0s-metallb-pool in this configuration, it will call the ‚Äúk0s-metallb-pool‚Äù to handle the IP communication in L2 (Layer 2) way, so that the traffic can be go-in to the cluster and go-out of the cluster\nNow we have all the requirement files, let‚Äôs apply it to our cluster using following command: # inside folder kustomize build . | kubectl apply -f - # outside folder kustomize build metallb-k0s/. | kubectl apply -f - #\u003c-- adjust to your folder name Before execute this command makesure you already have ‚Äúkubectl‚Äù and ‚Äúkustomize‚Äù installed on your local machine\nOnce applied, it should look like this capture:\nBefore move to the next part, let‚Äôs check if all the config already applied correctly with following commands:\n# In kubectl you can get multiple value from multiple sources like following kubectl get pods,deploy,ipaddresspool,l2advertisement -n metallb-system it should output as follows:\nas we can see the IP we set on ‚Äúipaddresspool.yaml‚Äù is properly configured and the ‚Äúl2advertisement.yaml‚Äù also pointing to the correct ‚Äúipaddresspool‚Äù name we set on the yaml file, but we are not sure yet if the configuration work expected or not, now let‚Äôs create simple nginx pods and services to check if MetalLB working as expected.\nlet‚Äôs prepare simple-nginx app like followings:\napiVersion: v1 kind: Pod metadata: name: simple-nginx namespace: default labels: app: simple-nginx spec: containers: - name: simple-nginx image: nginx:latest resources: limits: memory: \"128Mi\" cpu: \"500m\" requests: memory: \"128Mi\" cpu: \"500m\" --- apiVersion: v1 kind: Service metadata: name: simple-nginx-svc namespace: default spec: ipFamilies: - IPv4 ipFamliyPolicy: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: simple-nginx type: LoadBalancer then apply it with following command:\nkubectl apply -f e.g. kubectl apply -f simple-nginx.yaml the output should like following:\nonce applied, we can check the config is correctly applied with following command:\nkubectl get pods,svc -n default it should output something similar like following: as we can see from the ‚Äúservice‚Äù part, it have the load balancer type and configured with our allocated IP from the MetalLB IPaddressPool.\nnow the moment of truth, let‚Äôs try to access our simple-nginx app with the load balancer IP we have allocated above.\nit should be look like this on web or curl\nWhat to see on the next part ? Let‚Äôs summarize what we already do in this part as follows:\nWe learn to setup production ready K8s Cluster with Baremetal Load Balancer We test how the Baremetal Load Balancer work with simple nginx app We configure and setup the Baremetal Load Balancer with MetalLB Before I close this part, on the the next part we will learn about Ingress Controller and how it will help us with the application and the Load Balancer we already configured.\nI hope this part help you learn how to setup the production ready kubernetes cluster with K0s, see you on next post üòÑ.\n","wordCount":"1734","inLanguage":"en","datePublished":"2025-06-30T00:00:00Z","dateModified":"2025-06-30T00:00:00Z","author":[{"@type":"Person","name":"Dino"}],"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/jun-25/self-host-k8s-cluster-pt4/"},"publisher":{"@type":"Organization","name":"Dino-s26","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Dino-s26 (Alt + H)">Dino-s26</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title="About Me"><span>About Me</span></a></li><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/></a></div><h1 class="post-title entry-hint-parent">Self-host Kubernetes Cluster Pt.4</h1><div class=post-meta><span title='2025-06-30 00:00:00 +0000 UTC'>June 30, 2025</span>&nbsp;¬∑&nbsp;Dino</div></header><div class=post-content><h2 id=intermezzo>Intermezzo<a hidden class=anchor aria-hidden=true href=#intermezzo>#</a></h2><p><img alt=Intermezzo loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/self-host-kubernetes-pt4.png></p><p>Meowdy &#x1f63a;, been while not update the article series and welcome to the 4th Part of self-host kubernetes !</p><p>This part we will setup the Low spec Production read K8s Clusters (usually used for Edge / IoT cases). I was planned to use microk8s by ubuntu but since the k0s (K-Zero-S) just graduate from incubation, I would like to try that one</p><p>Before we start, here are the specs I use for the Server I used as the Control-Plane and worker</p><ul><li>Ubuntu Minimized, Version 22.04.5</li><li>Storage 20GB</li><li>RAM 2GB with 4GB Swap</li><li><a href=https://github.com/k0sproject/k0sctl#installation>k0sctl</a></li></ul><h2 id=install-and-configure-k0s-cluster>Install and Configure k0s Cluster<a hidden class=anchor aria-hidden=true href=#install-and-configure-k0s-cluster>#</a></h2><p>I assume you already know how to prepare the server on your own, so I will just jump to the installation part here, so let&rsquo;s start with installing the <strong>k0sctl</strong> tool, which we will use to bootstrap our cluster.</p><ol><li><p>Head to the Github of k0s to install the <a href=https://github.com/k0sproject/k0sctl#installation>k0sctl</a></p></li><li><p>SSH to your server and do update first before proceed with installation os k0s, since I use ubuntu usually I do as follows:</p></li></ol><pre tabindex=0><code>sudo apt update &amp;&amp; sudo apt upgrade -y

## for minimized version usually you need to 
install additional tools like nano, curl, etc etc ... 
so dont forget to install it 

sudo apt install nano curl -y 
</code></pre><ol start=3><li>Wait for the k0s installation complete, should the installation complete will return similar output as follow:</li></ol><p><img alt="k0s installation complete" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/k0s-install-complete.png></p><ol start=4><li>Next we will create bootstrap file for the controller, as follows:</li></ol><pre tabindex=0><code>mkdir -p /etc/k0s
k0s config create &gt; /etc/k0s/k0s.yaml

## as note, since we create file on /etc/ folder which manage by root, 
change to root user first before create the k0s.yaml 
or simply put the k0s.yaml somewhere first 
then copy with sudo for easier way of executable
</code></pre><ol start=5><li>Once the k0s.yaml config file ready, you can check some of the configuration first, as follows:</li></ol><pre tabindex=0><code>apiVersion: k0s.k0sproject.io/v1beta1
kind: ClusterConfig
metadata:
  name: k0s
  namespace: kube-system
spec:
  api:
    address: 192.168.1.81
    ca:
      certificatesExpireAfter: 8760h0m0s
      expiresAfter: 87600h0m0s
    k0sApiPort: 9443
    port: 6443
    sans:
    - 192.168.1.81
    - fe80::be24:11ff:fe41:f0a0
    - k0s.&lt;domain.tld&gt; ## custom sans I add here 
  controllerManager: {}
  extensions:
    helm:
      concurrencyLevel: 5
  installConfig:
    users:
      etcdUser: etcd
      kineUser: kube-apiserver
      konnectivityUser: konnectivity-server
      kubeAPIserverUser: kube-apiserver
      kubeSchedulerUser: kube-scheduler
  konnectivity:
    adminPort: 8133
    agentPort: 8132
  network:
    clusterDomain: cluster.local
    dualStack:
      enabled: false
    kubeProxy:
      iptables:
        minSyncPeriod: 0s
        syncPeriod: 0s
      ipvs:
        minSyncPeriod: 0s
        syncPeriod: 0s
        tcpFinTimeout: 0s
        tcpTimeout: 0s
        udpTimeout: 0s
      metricsBindAddress: 0.0.0.0:10249
      mode: iptables
      nftables:
        minSyncPeriod: 0s
        syncPeriod: 0s
    kuberouter:
      autoMTU: true
      hairpin: Enabled
      metricsPort: 8080
    nodeLocalLoadBalancing:
      enabled: false
      envoyProxy:
        apiServerBindPort: 7443
        konnectivityServerBindPort: 7132
      type: EnvoyProxy
    podCIDR: 10.244.0.0/16
    provider: kuberouter
    serviceCIDR: 10.96.0.0/12
  scheduler: {}
  storage:
    etcd:
      ca:
        certificatesExpireAfter: 8760h0m0s
        expiresAfter: 87600h0m0s
      peerAddress: 192.168.1.81
    type: etcd
  telemetry:
    enabled: false
</code></pre><p>on step 4 I just make adjustment to add additional SANS, since I will have the cluster be accessable with domain I add, I still not sure will it work or not since this is the first time I setup the k0s Cluster</p><ol start=6><li>Once the configuratio is generated and adjusted, let&rsquo;s start the control-plane with following command:</li></ol><pre tabindex=0><code>sudo k0s install controller -c /etc/k0s/k0s.yaml --enable-worker --no-taints
sudo k0s start
</code></pre><p>you will see nothing on your terminal, but if you do see some error, try to check the error first before continue with next steps</p><ol start=7><li>Let&rsquo;s verify if the installation really complete with following commands:</li></ol><pre tabindex=0><code>sudo k0s status
</code></pre><p>you will see similar output as follow:</p><p><img alt="k0s status" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/k0s-status.png></p><ol start=8><li>Okay, so now we have the control-plane configured and running, let&rsquo;s verify some of the kubernetes services with following commands:</li></ol><pre tabindex=0><code>sudo k0s kubectl get nodes -A
sudo k0s kubectl cluster-info
sudo k0s kubectl get all -A -owide
</code></pre><p>it should output something similar as follows:</p><p><img alt="k0s kubernetes component" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/k0s-kubernetes-components.png></p><p>as we can see the cluster is already properly setup, now let&rsquo;s get the kubeconfig to be use from our local machine</p><ol start=9><li>To get the kubeconfig from k0s, use the following command:</li></ol><pre tabindex=0><code>sudo k0s kubeconfig admin
</code></pre><p>it will output the kubeconfig configuration to your terminal, the put is somehere in your local machine (e.g. ~/.kube/config)</p><ol start=10><li>Test the kubectl from your local machine after you update the kubeconfig, you should be able to access it like this</li></ol><p><img alt="kubectl from local machine" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/kubectl-local-machine.png></p><h2 id=install-load-balancer>Install Load Balancer<a hidden class=anchor aria-hidden=true href=#install-load-balancer>#</a></h2><p>So we just finished installed our K8s Cluster with K0s, but we dont actually finished yet with the installation, usually if we do setup on the cloud provider we will have the load balancer automatically provisioned to our cluster, but not quite on the self-host setup.</p><p>Now, let&rsquo;s configure Load Balancer to our K8s cluster, we will use MetalLB as our Baremetal Load Balancer for L2 Traffic that will go-in and go-out of our k8s cluster.</p><ol><li>First let&rsquo;s create new namespace called <strong>&ldquo;metallb-system&rdquo;</strong>, once created we will deploy the metallb with kustomize in order for easier management of the resource we will do, use the following command to create the namespace:</li></ol><pre tabindex=0><code>kubectl create namespace metallb-system

## To verify the namespace creation use following command

kubectl get namespaces
</code></pre><p>the output should be similar like following capture:</p><p><img alt="Kubectl create namespace metallb" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/kubectl-create-ns-metallb.png></p><ol start=2><li>Next we will create folder containing our kustomize resources will be deployed, in this case for metallb requirement, following are the required resource will be created:</li></ol><pre tabindex=0><code>metallb-k0s/
‚îú‚îÄ‚îÄ ipaddresspool.yaml
‚îú‚îÄ‚îÄ kustomization.yaml
‚îî‚îÄ‚îÄ l2advertisement.yaml
</code></pre><p>Before continue, as we can see we have 3 yaml files, which are <strong>&lsquo;ipaddresspool.yaml, kustomization.yaml, l2advertisement.yaml&rsquo;</strong>, let me explain a bit why we need these 3 files to install metallb to our cluster &mldr;</p><ul><li><strong>[ kustomization.yaml ]</strong></li></ul><p>This file will read all our customization we need in-order to install the MetalLB to our cluster, which we will break down in a bit how the structure looks like as follows:</p><pre tabindex=0><code>### kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
namespace: metallb-system

resources:
  - github.com/metallb/metallb/config/native?ref=v0.15.2
  - ipaddresspool.yaml
  - l2advertisement.yaml
</code></pre><p>as we can see this is the structure for <strong>kustomization.yaml</strong>, which we will call necessary resources like the metallb installation resources, <strong>ipaddresspool.yaml</strong> and also <strong>l2advertisement.yaml</strong></p><ul><li><strong>[ ipaddresspool.yaml ]</strong></li></ul><p>This file will be our allocation IP Address used for the Load Balancer, it can be IP Public or IP Private, depending on your setup, but for this case since this is self-hosted on my home lab, I will be using IP Private, the structure looks likes as follwos:</p><pre tabindex=0><code>apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: k0s-metallb-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.1.85-192.168.1.90 #&lt;-- kindly change to your IP Address allocation
  autoAssign: true
  avoidBuggyIPs: true
</code></pre><p>as we can see this is the structure of <strong>ipaddresspool.yaml</strong>, in this case we allocate IP Private from <em><strong>192.168.1.85 - 192.168.1.90</strong></em>, also we assign the name of the IPAddressPool and namespace where we will deploy it.</p><p>there will be question, why there is <strong>&ldquo;autoAssign&rdquo; and &ldquo;avoidBuggyIPs&rdquo;</strong> in the configuration.</p><p><strong>&ldquo;autoAssign&rdquo;</strong> is used when there is <strong>&ldquo;services&rdquo;</strong> that configured as LoadBalancer Type, for example like following:</p><pre tabindex=0><code>apiVersion: v1
kind: Service

metadata: 
  name: nginx-ingress-controller
  namespace: nginx-ingress

spec: 
  allocateLoadBalancerNodePorts: true
  ClusterIP: x.x.x.x
  ClusterIPs: 
  - x.x.x.x
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies: 
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: web
    nodePort: 32732
    port: 80
    protocol: TCP
    targetPort: web
  - name: websecure
    nodePort: 32180
    port: 443
    protocol: TCP
    targetPort: websecure
  selector: 
    app.kubernetes.io/name: nginx-ingress-controller
  type: LoadBalancer # &lt;-- This is the part MetalLB will see when assigning IP
</code></pre><p>This is only example what will be check by MetalLB before auto assign the IP Address, we can also allocate it manually but will need some manual adjustment to it, so for now let&rsquo;s leave it as <strong>&ldquo;autoAssign&rdquo;</strong></p><p>for the <strong>&ldquo;avoidBuggyIPs&rdquo;</strong>, it is related to the network devices that have protection against Smurf IP Address like &ldquo;.0&rdquo; or &ldquo;.255&rdquo;, the detail can be read <a href=https://metallb.io/configuration/_advanced_ipaddresspool_configuration/#handling-buggy-networks>here</a></p><ul><li><strong>[ l2advertisement.yaml ]</strong></li></ul><p>This file will handle all the L2 communication going-in and going-out of the cluster, since we don&rsquo;t use BGP for this setup, we only configure L2-Advertisement only, here are the structure looks like:</p><pre tabindex=0><code>apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: k0s-metallb-l2adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - k0s-metallb-pool
</code></pre><p>in this configuration, it will call the <strong>&ldquo;k0s-metallb-pool&rdquo;</strong> to handle the IP communication in L2 (Layer 2) way, so that the traffic can be go-in to the cluster and go-out of the cluster</p><ol start=3><li>Now we have all the requirement files, let&rsquo;s apply it to our cluster using following command:</li></ol><pre tabindex=0><code># inside folder
kustomize build . | kubectl apply -f -

# outside folder
kustomize build  metallb-k0s/. | kubectl apply -f -  #&lt;-- adjust to your folder name
</code></pre><p>Before execute this command makesure you already have <strong>&ldquo;kubectl&rdquo;</strong> and <strong>&ldquo;kustomize&rdquo;</strong> installed on your local machine</p><p>Once applied, it should look like this capture:</p><p><img alt="kustomization apply" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/kustomization-apply-metallb.png></p><p>Before move to the next part, let&rsquo;s check if all the config already applied correctly with following commands:</p><pre tabindex=0><code># In kubectl you can get multiple value from multiple sources like following

kubectl get pods,deploy,ipaddresspool,l2advertisement -n metallb-system 
</code></pre><p>it should output as follows:</p><p><img alt="kubectl get" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/kubectl-get-output-metallb.png></p><p>as we can see the IP we set on <strong>&ldquo;ipaddresspool.yaml&rdquo;</strong> is properly configured and the <strong>&ldquo;l2advertisement.yaml&rdquo;</strong> also pointing to the correct <strong>&ldquo;ipaddresspool&rdquo;</strong> name we set on the yaml file, but we are not sure yet if the configuration work expected or not, now let&rsquo;s create simple nginx pods and services to check if MetalLB working as expected.</p><p>let&rsquo;s prepare simple-nginx app like followings:</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: simple-nginx
  namespace: default
  labels:
    app: simple-nginx
spec:
  containers:
  - name: simple-nginx
    image: nginx:latest
    resources:
      limits:
        memory: &#34;128Mi&#34;
        cpu: &#34;500m&#34;
      requests:
        memory: &#34;128Mi&#34;
        cpu: &#34;500m&#34;
---
apiVersion: v1
kind: Service
metadata:
  name: simple-nginx-svc
  namespace: default
spec:
  ipFamilies:
  - IPv4
  ipFamliyPolicy:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: simple-nginx
  type: LoadBalancer
</code></pre><p>then apply it with following command:</p><pre tabindex=0><code>kubectl apply -f &lt;your yaml file name&gt;

e.g. kubectl apply -f simple-nginx.yaml
</code></pre><p>the output should like following:</p><p><img alt="kubectl apply simple-nginx" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/k-apply-simple-nginx.png></p><p>once applied, we can check the config is correctly applied with following command:</p><pre tabindex=0><code>kubectl get pods,svc -n default
</code></pre><p>it should output something similar like following:
<img alt="kubectl get pods,svc -n default" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/simple-nginx-app.png></p><p>as we can see from the <strong>&ldquo;service&rdquo;</strong> part, it have the load balancer type and configured with our allocated IP from the MetalLB IPaddressPool.</p><p>now the moment of truth, let&rsquo;s try to access our simple-nginx app with the load balancer IP we have allocated above.</p><p>it should be look like this on web or curl</p><p><img alt="web access simple-nginx" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/web-access-simple-nginx.png>
<img alt="curl simple-nginx" loading=lazy src=/img/jun-25/self-host-kubernetes-pt4/curl-simple-nginx.png></p><h2 id=what-to-see-on-the-next-part->What to see on the next part ?<a hidden class=anchor aria-hidden=true href=#what-to-see-on-the-next-part->#</a></h2><p>Let&rsquo;s summarize what we already do in this part as follows:</p><ol><li>We learn to setup production ready K8s Cluster with Baremetal Load Balancer</li><li>We test how the Baremetal Load Balancer work with simple nginx app</li><li>We configure and setup the Baremetal Load Balancer with MetalLB</li></ol><p>Before I close this part, on the the next part we will learn about Ingress Controller and how it will help us with the application and the Load Balancer we already configured.</p><p>I hope this part help you learn how to setup the production ready kubernetes cluster with K0s, see you on next post &#x1f604;.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/kubernetes/>Kubernetes</a></li><li><a href=http://localhost:1313/tags/k8s/>K8s</a></li><li><a href=http://localhost:1313/tags/microk8s/>Microk8s</a></li><li><a href=http://localhost:1313/tags/self-host/>Self-Host</a></li><li><a href=http://localhost:1313/tags/tech/>Tech</a></li><li><a href=http://localhost:1313/tags/learning/>Learning</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/jun-25/self-host-k8s-cluster-pt3/><span class=title>Next ¬ª</span><br><span>Self-host Kubernetes Cluster Pt.3</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Self-host Kubernetes Cluster Pt.4 on x" href="https://x.com/intent/tweet/?text=Self-host%20Kubernetes%20Cluster%20Pt.4&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fjun-25%2fself-host-k8s-cluster-pt4%2f&amp;hashtags=kubernetes%2ck8s%2cmicrok8s%2cself-host%2ctech%2clearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Self-host Kubernetes Cluster Pt.4 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fjun-25%2fself-host-k8s-cluster-pt4%2f&amp;title=Self-host%20Kubernetes%20Cluster%20Pt.4&amp;summary=Self-host%20Kubernetes%20Cluster%20Pt.4&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fjun-25%2fself-host-k8s-cluster-pt4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Self-host Kubernetes Cluster Pt.4 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fjun-25%2fself-host-k8s-cluster-pt4%2f&title=Self-host%20Kubernetes%20Cluster%20Pt.4"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Self-host Kubernetes Cluster Pt.4 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fjun-25%2fself-host-k8s-cluster-pt4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Self-host Kubernetes Cluster Pt.4 on whatsapp" href="https://api.whatsapp.com/send?text=Self-host%20Kubernetes%20Cluster%20Pt.4%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fjun-25%2fself-host-k8s-cluster-pt4%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Self-host Kubernetes Cluster Pt.4 on telegram" href="https://telegram.me/share/url?text=Self-host%20Kubernetes%20Cluster%20Pt.4&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fjun-25%2fself-host-k8s-cluster-pt4%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Self-host Kubernetes Cluster Pt.4 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Self-host%20Kubernetes%20Cluster%20Pt.4&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fjun-25%2fself-host-k8s-cluster-pt4%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Dino-s26</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>